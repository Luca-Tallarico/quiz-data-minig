### PARTE 1: INTRODUZIONE E processo KDP

1. Qual è la relazione tra il *Knowledge Discovery Process (KDP)* e il *Data Mining*?
   A. I due termini sono sostanzialmente intercambiabili e vengono usati indistintamente sia in ambito accademico che industriale.
   B. Il Data Mining comprende l'intero flusso di lavoro, dalla pulizia dei dati alla visualizzazione, mentre il KDP è limitato alla sola parte teorica.
   C. Il KDP è l'intero processo strutturato (Cleaning, Integration, ecc.), di cui il Data Mining è solo una fase specifica (tipicamente lo step 4/5).
   D. Il KDP è una metodologia specifica per i Data Warehouse relazionali, mentre il Data Mining si riferisce esclusivamente all'analisi di dati non strutturati.

   **Spiegazione:** Data Mining è una fase specifica all'interno del processo più ampio di Knowledge Discovery Process (KDP), che include anche cleaning, integration e post-processing.

2. In quale fase del processo KDP avviene la rimozione del *Noise* e dei dati inconsistenti?
   A. Pattern Evaluation, dove si filtrano i risultati inutili.
   B. Data Cleaning, per eliminare errori e outlier.
   C. Data Transformation, durante la normalizzazione dei valori.
   D. Data Integration, unendo diverse sorgenti dati.

   **Spiegazione:** I primi step del KDP sono, in ordine: Data Cleaning, Integration, Selection, Transformation (prima del Data Mining vero e proprio).

3. Cosa si intende per *Bias* nei dati in questo contesto?
   A. Una distorsione sistematica o pregiudizio (es. sociale, razziale, o di campionamento) che influenza la rappresentatività dei dati.
   B. La varianza statistica intrinseca di un modello di regressione non lineare.
   C. Un particolare tipo di rumore casuale ad alta frequenza, simile all'effetto "sale e pepe" nelle immagini digitali.
   D. Un errore di sintassi generato durante l'importazione dei dati nel DBMS.

   **Spiegazione:** Nel documento "APPUNTI LEZIONE", si distingue nettamente tra **Rumore (noise)** e **Bias**. Il **Rumore** viene descritto, ad esempio, come "Rumore sale e pepe (pixel bianchi e neri sull'immagine)" che può essere cancellato con filtri (denoising). Il **Bias** è invece definito come "pregiudizi" (es. di genere) riflessi nei dati, non un disturbo a livello di pixel. Pertanto, l'image denoising agisce sul noise (pixel), non sul bias. 

4. Quale delle seguenti è la definizione più appropriata di Data Mining?
   A. L'inserimento manuale e la correzione di record in fogli di calcolo.
   B. Il processo automatizzato di esplorazione di grandi moli di dati per individuare pattern, correlazioni e strutture non banali.
   C. La procedura di backup e ripristino dei database aziendali per garantire la sicurezza.
   D. La semplice rappresentazione grafica dei dati tramite istogrammi e diagrammi a torta per reportistica.

   **Spiegazione:** Data Mining è definito come l'estrazione automatizzata (o semi-automatizzata) di pattern, conoscenze e relazioni nascoste da grandi moli di dati.

5. I dati strutturati sono caratterizzati da:
   A. Testo libero privo di qualsiasi formattazione o metadato.
   B. Formati flessibili come JSON o XML che usano tag ma non impongono uno schema rigido.
   C. Un'organizzazione rigida in tabelle con righe e colonne definite da uno schema fisso (es. RDBMS).
   D. Contenuti multimediali complessi come flussi video e archivi di immagini.

   **Spiegazione:** XML e JSON sono tipici esempi di dati Semi-strutturati: hanno una struttura interna (tag) ma non uno schema fisso e rigido come le tabelle relationali.

6. Qual è l'obiettivo principale della fase di *Data Integration*?
   A. Suddividere il dataset in Training Set e Test Set per la validazione.
   B. Comprimere i file di dati per ottimizzare lo spazio di archiviazione su disco.
   C. Discretizzare le variabili continue in intervalli categorici per l'analisi.
   D. Unificare e combinare dati provenienti da molteplici sorgenti eterogenee in un unico deposito coerente.

   **Spiegazione:** La Data Integration serve a combinare dati da diverse fonti eterogenee (DB, file, API) in un unico repository coerente per l'analisi.

7. Un *Pattern* nel contesto del Data Mining è definito come:
   A. Un comando SQL utilizzato per estrarre record specifici dal database.
   B. Una struttura, modello o sequenza ricorrente che esibisce una regolarità sistematica all'interno dei dati.
   C. Un valore anomalo o errato che deve essere rimosso durante il cleaning.
   D. L'attributo che funge da chiave primaria in una tabella relazionale.

   **Spiegazione:** Un Pattern è una struttura ricorrente o una relazione sistematica osservabile nei dati, che il Data Mining cerca di identificare.

8. Quale categoria di dati include tipicamente file XML o JSON?
   A. Dati Binari (immagini, eseguibili)
   B. Dati Semi-strutturati (con tag/marcatori ma senza schema rigido)
   C. Dati Non Strutturati (testo libero, audio)
   D. Dati Strutturati (tabelle relazionali)

   **Spiegazione:** I file HTML (così come XML e JSON) sono considerati **Dati Semi-strutturati** perché possiedono una struttura organizzativa tramite tag che separano gli elementi semantici, pur contenendo testo libero e non seguendo uno schema rigido come i database relazionali. 

9. Qual è la distinzione chiave tra Data Mining *Predittivo* e *Descrittivo*?
   A. Il Predittivo utilizza esclusivamente linguaggi di query come SQL, mentre il Descrittivo richiede programmazione in Python o R.
   B. Il Predittivo mira a costruire modelli per stimare valori futuri o sconosciuti, mentre il Descrittivo cerca di interpretare e trovare pattern nei dati attuali.
   C. In realtà non esiste alcuna differenza sostanziale, sono solo terminologie diverse per indicare le stesse tecniche di analisi statistica.
   D. Il Predittivo si occupa di riassumere le performance storiche passate, mentre il Descrittivo proietta scenari ipotetici nel futuro.

   **Spiegazione:** Il Data Mining Predittivo usa il passato per stimare il futuro/sconosciuto; il Descrittivo cerca di caratterizzare le proprietà dei dati attuali.

10. Qual è la sequenza logica tipica dei passi nel processo KDP?
    A. Evaluation -> Data Mining -> Transformation -> Cleaning -> Integration
    B. Data Mining -> Cleaning -> Integration -> Evaluation -> Selection
    C. Selection -> Data Mining -> Cleaning -> Integration -> Transformation
    D. Cleaning -> Integration -> Transformation -> Data Mining -> Evaluation

   **Spiegazione:** La sequenza logica standard è: Cleaning -> Integration -> Selection -> Transformation -> Data Mining -> Evaluation -> Presentation.

### PARTE 2: DATABASE E DATA WAREHOUSE

11. Che cos'è un *DBMS* (Database Management System)?
    A. Un dispositivo hardware specializzato per l'archiviazione ad alta velocità.
    B. Un algoritmo avanzato per il clustering di dati multidimensionali.
    C. Un software di sistema progettato per definire, manipolare, recuperare e gestire i dati in un database.
    D. Un formato di file compresso utilizzato per i backup.

   **Spiegazione:** Il **Dendrogramma** è il diagramma ad albero fondamentale e distintivo utilizzato per visualizzare i risultati del **Clustering Gerarchico** (Hierarchical Clustering). Mostra come i cluster vengono uniti (approccio agglomerativo) o divisi (approccio divisivo) passa dopo passo. K-means e DBSCAN non producono dendrogrammi nativamente. 

12. Qual è la funzione primaria di un *Data Warehouse*?
    A. Supportare i processi decisionali e la Business Intelligence (OLAP) integrando dati storici da varie fonti.
    B. Gestire le transazioni operative quotidiane (OLTP) con la massima velocità e consistenza.
    C. Archiviare esclusivamente dati non strutturati come documenti, email e file multimediali.
    D. Fornire un'estensione virtuale della memoria RAM per i server di calcolo.

   **Spiegazione:** Il Data Warehouse è ottimizzato per l'analisi (OLAP) e il supporto alle decisioni, integrando dati storici, a differenza dei DB operazionali (OLTP).

13. Quale elemento garantisce l'unicità di ogni record (tupla) in una tabella relazionale?
    A. La Foreign Key (Chiave Esterna).
    B. Un qualsiasi attributo numerico (es. Età).
    C. La Primary Key (Chiave Primaria).
    D. Il nome assegnato alla tabella nel database.

   **Spiegazione:** La Chiave Primaria (Primary Key) è l'attributo (o set di attributi) che identifica in modo univoco ogni record in una tabella.

14. Come si definisce un *Data Lake*?
    A. Un piccolo database dipartimentale (Data Mart) specifico per un singolo team.
    B. Un vasto repository che immagazzina grandi quantità di dati grezzi nel loro formato nativo (strutturati e non).
    C. Un dashboard grafico per la visualizzazione dei flussi di dati in tempo reale.
    D. Un sistema di backup su nastro per l'archiviazione a lungo termine.

   **Spiegazione:** Il Data Lake è un repository che raccoglie grandi quantità di dati grezzi nel loro formato originale, senza imporre uno schema a priori (schema-on-read).

15. Quale operazione OLAP consente di aumentare il livello di dettaglio dei dati visualizzati (es. passando dalle vendite annuali a quelle mensili)?
    A. Drill-down
    B. Pivot (Rotazione)
    C. Roll-up
    D. Slice and Dice

   **Spiegazione:** Il Drill-down è l'operazione di 'zoom-in' che permette di visualizzare i dati con maggiore dettaglio (es. da anno a mese).

16. In cosa differisce un *Data Mart* da un Enterprise Data Warehouse?
    A. È focalizzato su un sottoinsieme specifico di dati aziendali (es. un dipartimento), piuttosto che sull'intera organizzazione.
    B. Contiene l'integrazione completa di tutti i dati aziendali di tutte le filiali globali.
    C. Non supporta l'esecuzione di query SQL standard per l'analisi.
    D. È progettato esclusivamente per contenere dati non strutturati come video e immagini.

   **Spiegazione:** Un Data Mart è un sottoinsieme del Data Warehouse focalizzato su un'area specifica o dipartimento (es. Marketing), per un accesso più rapido.

17. Nella query SQL `SELECT * FROM Customers WHERE country = 'USA'`, la clausola `WHERE` serve a:
    A. Eseguire un join con una tabella esterna.
    B. Ordinare i risultati in base a un criterio alfabetico o numerico.
    C. Filtrare le righe restituite, includendo solo quelle che soddisfano la condizione specificata.
    D. Selezionare quali colonne visualizzare nel risultato finale.

   **Spiegazione:** Il termine "Feedforward" significa letteralmente "che alimenta in avanti". In queste reti, l'informazione viaggia solo in una direzione: dall'input, attraverso i layer nascosti (hidden), fino all'output. Non ci sono cicli o loop. L'opzione B afferma che l'informazione si propaga *all'indietro*, il che è falso per la fase di elaborazione (inference). La propagazione all'indietro (backpropagation) avviene solo durante l'addestramento per aggiornare i pesi, ma non definisce il flusso dell'informazione nella rete feedforward stessa. Essendo una domanda che chiede cosa NON è corretto, la B è la risposta giusta. 

18. Quale tipo di *JOIN* restituisce tutte le righe della tabella di sinistra, anche se non vi è corrispondenza nella tabella di destra?
    A. INNER JOIN (solo corrispondenze esatte)
    B. LEFT JOIN (tutto a sinistra + match a destra o NULL)
    C. FULL JOIN (tutto da entrambe le tabelle)
    D. RIGHT JOIN (tutto a destra + match a sinistra o NULL)

   **Spiegazione:** Gli appunti affermano esplicitamente che i pattern possono emergere e essere trovati in entrambe le tipologie di dati: sia **dati strutturati** (database relazionali, tabelle) sia **dati non strutturati** (testi, immagini, flussi log). 

19. Qual è il ruolo di una *Foreign Key* in un database relazionale?
    A. Criptare i dati sensibili per garantire la sicurezza delle informazioni.
    B. Eliminare automaticamente i record duplicati o obsoleti.
    C. Identificare univocamente un record all'interno della sua stessa tabella.
    D. Creare un collegamento logico riferendosi alla Primary Key di un'altra tabella.

   **Spiegazione:** Una RNN (Recurrent Neural Network) è adatta per dati sequenziali (come il testo o serie temporali) grazie alla sua memoria interna.

20. Cosa contengono tipicamente i dati di un *Transactional Database*?
    A. Dettagli operativi come ID transazione, data, e lista degli articoli acquistati.
    B. Informazioni puramente anagrafiche e statiche sui dipendenti.
    C. Report statistici aggregati su base annuale per il management.
    D. Collezioni di file multimediali non strutturati.

   **Spiegazione:** I dati strutturati (Structured Data) seguono un modello rigido, tipicamente organizzati in righe e colonne come nei database relazionali (RDBMS).

### PARTE 3: CLUSTERING (Unsupervised Learning)

21. Il *Clustering* appartiene alla categoria:
    A. Reinforcement Learning (Apprendimento per Rinforzo)
    B. Database Management & Administration
    C. Supervised Learning (Apprendimento Supervisionato)
    D. Unsupervised Learning (Apprendimento Non Supervisionato)

   **Spiegazione:** Il Clustering raggruppa dati simili in insiemi (cluster) senza avere etichette predefinite (apprendimento non supervisionato).

22. Nel contesto dell'*Unsupervised Learning*, quale elemento fondamentale manca rispetto all'apprendimento supervisionato?
    A. La potenza di calcolo necessaria per processare i dati.
    B. Gli attributi (features) descrittivi dei dati.
    C. Le etichette di classe (target labels) o la "Ground Truth" per guidare l'apprendimento.
    D. I dati di input grezzi su cui effettuare l'analisi.

   **Spiegazione:** I fogli di calcolo (spreadsheet) sono considerati dati "Strutturati" (o talvolta semi-strutturati a seconda della rigidità), ma nel contesto del corso e degli appunti, vengono citati esplicitamente: "Structured Data: Data with high degree of organisation (in a spreadsheet-like manner)". Pertanto la risposta corretta è Strutturati. 

23. Qual è l'obiettivo fondamentale di un algoritmo di Clustering?
    A. Stimare con precisione un valore numerico continuo futuro.
    B. Identificare ed eliminare sistematicamente tutti gli outlier dal dataset.
    C. Assegnare i dati a classi predefinite note a priori dal dominio.
    D. Massimizzare la similarità tra oggetti nello stesso gruppo e minimizzarla tra gruppi diversi.

   **Spiegazione:** K-Means è un algoritmo di clustering a partizionamento che divide i dati in K gruppi minimizzando la varianza interna ai gruppi.

24. Nell'algoritmo *K-means*, cosa indica il parametro 'K'?
    A. La distanza massima consentita tra due punti nello stesso cluster.
    B. Il tasso di apprendimento (learning rate) dell'algoritmo.
    C. Il numero di cluster che l'algoritmo deve formare, deciso a priori dall'utente.
    D. Il numero massimo di iterazioni prima che l'algoritmo si arresti.

   **Spiegazione:** Nel K-Means, 'K' rappresenta il numero di cluster che l'utente deve definire a priori.

25. Qual è uno dei principali limiti o svantaggi dell'algoritmo *K-means*?
    A. È applicabile esclusivamente a dataset con esattamente due cluster naturali.
    B. Richiede di specificare K a priori e tende a funzionare male con cluster di forma non sferica o dimensioni molto diverse.
    C. Ha una complessità computazionale così elevata da renderlo inutilizzabile su dataset moderni.
    D. Non è in grado di processare alcun tipo di dato numerico.

   **Spiegazione:** Tra i principali metodi di clustering, il **Clustering Gerarchico** (Hierarchical Clustering) è quello che si divide in due famiglie, descritte negli appunti come: - 1. **Agglomerativo (Agglomerative)**: Approccio *bottom-up*, dove si parte da tanti cluster quanti sono i punti e si uniscono. - 2. **Divisivo (Divisive)**: Approccio *top-down*, dove si parte da un unico cluster e lo si divide ricorsivamente. Il K-means è un clustering a partizionamento, mentre DBSCAN è basato sulla densità. 

26. Come procede il *Clustering Gerarchico Agglomerativo* (Bottom-up)?
    A. Richiede obbligatoriamente di definire il numero finale di cluster K prima di iniziare.
    B. Utilizza dei centroidi mobili che vengono ricalcolati iterativamente come nel K-means.
    C. Parte da un unico grande cluster contenente tutti i dati e lo suddivide ricorsivamente.
    D. Inizia trattando ogni punto come un cluster singolo e unisce iterativamente le coppie più vicine.

   **Spiegazione:** Il 'Centroid' è il punto centrale (media) di un cluster nel K-Means.

27. Quale informazione fornisce un *Dendrogramma*?
    A. Una rappresentazione della distribuzione probabilistica delle variabili.
    B. La visualizzazione della gerarchia dei cluster e delle distanze a cui avvengono le unioni.
    C. Lo schema dell'architettura della rete neurale utilizzata.
    D. L'andamento temporale delle metriche di vendita nel tempo.

   **Spiegazione:** Il Dendrogramma è il grafico ad albero usato per visualizzare la gerarchia dei cluster nel Clustering Gerarchico.

28. Nel metodo di Ward (Ward's linkage), la distanza tra due cluster viene calcolata per:
    A. Minimizzare l'incremento della varianza interna al cluster (Sum of Squared Errors) risultante dall'unione.
    B. Massimizzare la distanza tra i due membri più lontani dei rispettivi cluster.
    C. Minimizzare la distanza tra i due membri più vicini dei rispettivi cluster.
    D. Calcolare semplicemente la media aritmetica delle distanze tra tutti i punti.

   **Spiegazione:** Il testo conferma che CBOW (Continuous Bag of Words) è una delle due architetture principali utilizzate dalla tecnica **word2vec** (l'altra è skip-gram). 

29. Su quale principio si basa l'algoritmo *DBSCAN* per formare i cluster?
    A. Sulla densità dei punti in una regione dello spazio (Density-Based Clustering).
    B. Sulla minimizzazione della distanza dai centroidi (Centroid-Based).
    C. Su regole decisionali gerarchiche predefinite (Tree-Based).
    D. Sull'utilizzo di etichette fornite manualmente dagli utenti.

   **Spiegazione:** DBSCAN è un algoritmo di clustering basato sulla densità, capace di trovare cluster di forma arbitraria e gestire il rumore.

30. In DBSCAN, come viene etichettato un punto che non ha sufficienti vicini nel raggio Epsilon e non è raggiungibile da un Core Point?
    A. Border Point (Punto di Confine)
    B. Centroid (Centroide del cluster)
    C. Core Point (Punto Centrale)
    D. Noise Point (Rumore/Outlier)

31. Qual è un vantaggio significativo di DBSCAN rispetto a K-means?
    A. Richiede necessariamente di conoscere il numero di cluster K in anticipo.
    B. È computazionalmente più veloce su dataset estremamente piccoli e semplici.
    C. Tende a trovare esclusivamente cluster di forma sferica o convessa.
    D. È capace di individuare cluster di forma arbitraria e di gestire efficacemente il rumore (outlier).

   **Spiegazione:** Big Data si riferisce a dataset così grandi, veloci o complessi (Volume, Velocity, Variety) da richiedere tecnologie specifiche oltre i tradizionali RDBMS.

32. L'algoritmo K-means garantisce di convergere a:
    A. Un ottimo locale, che dipende fortemente dalla scelta iniziale dei centroidi.
    B. La corretta densità di distribuzione dei dati originali.
    C. L'ottimo globale assoluto, indipendentemente dall'inizializzazione.
    D. Il numero esatto di cluster naturali presenti nei dati.

   **Spiegazione:** Per generare etichette da dati non classificati ('uncategorised'), si usa il Clustering Analysis per trovare raggruppamenti naturali.

33. Quale metrica di distanza è comunemente utilizzata nell'implementazione standard del K-means?
    A. Distanza di Manhattan (L1).
    B. Distanza Euclidea (L2).
    C. Similarità del Coseno.
    D. Coefficiente di Jaccard.

   **Spiegazione:** La Cosine Similarity misura la similarità tra due vettori (es. documenti) calcolando il coseno dell'angolo tra loro.

34. Cosa indica il termine *Ground Truth* nel Machine Learning?
    A. I dati grezzi originali prima di qualsiasi elaborazione o pulizia.
    B. L'infrastruttura hardware fisica (server) su cui vengono eseguiti i calcoli.
    C. Le etichette reali e verificate ("verità di base") usate per valutare l'accuratezza di un modello.
    D. Un algoritmo specifico per la pulizia profonda dei dati.

   **Spiegazione:** La definizione nella domanda corrisponde esattamente alla descrizione di **Ground Truth** (verità di base) fornita nelle slide. Si riferisce ai valori o alle etichette verificate (etichette "label") usate come riferimento standard per addestrare e valutare i modelli. 

35. Nel *Single Linkage* (clustering gerarchico), come si misura la distanza tra due cluster?
    A. Come distanza tra i centroidi (punti medi) dei due cluster.
    B. Come media di tutte le distanze tra le coppie di punti dei due cluster.
    C. Come distanza tra i due punti più lontani appartenenti ai due cluster diversi.
    D. Come distanza tra i due punti più vicini appartenenti ai due cluster diversi.

   **Spiegazione:** Nel **Single Linkage** (o Nearest Neighbor), la distanza tra due cluster è definita come la distanza minima tra qualsiasi punto del primo cluster e qualsiasi punto del secondo. È come dire che due gruppi "si toccano" nel loro punto più vicino. Questo metodo tende a creare cluster allungati (effetto *chaining*). 

### PARTE 4: NEURAL NETWORKS & DEEP LEARNING

36. Il *Perceptron* ideato da Rosenblatt trae ispirazione da:
    A. I principi della meccanica quantistica applicata al calcolo.
    B. I processi metabolici del fegato umano.
    C. La teoria matematica dei grafi complessi.
    D. La struttura e il funzionamento del neurone biologico.

37. Quali sono i componenti strutturali chiave di un Perceptron classico?
    A. Unicamente un nodo di Input e un nodo di Output diretti.
    B. Una complessa struttura ad albero decisionale (Random Forest).
    C. Tabelle di database relazionali interconnesse.
    D. Input, Pesi (Weights), Bias, e una Funzione di Attivazione a soglia.

   **Spiegazione:** Il Teorema di convergenza del Perceptron (e quanto riportato specificamente in *Lecture 03*) stabilisce che l'algoritmo **garantisce la convergenza** (ossia di trovare un iperpiano di separazione e smettere di aggiornare i pesi) **solo se e quando** i dati di input sono **linearmente separabili**. Se i dati non lo sono, (senza accorgimenti come il limite di epoche) l'algoritmo continuerebbe a ciclare all'infinito cercando una soluzione che non esiste. 

38. A cosa serve la funzione di attivazione a soglia nel Perceptron originale?
    A. A convertire stringhe di testo in valori numerici.
    B. Ad eliminare i dati errati o corrotti dal dataset.
    C. A determinare l'output binario (es. +1/-1) verificando se la somma pesata supera una certa soglia.
    D. A calcolare la somma aritmetica semplice di tutti gli input senza pesi.

   **Spiegazione:** Il Bias rappresenta una distorsione sistematica (es. pregiudizio nei dati di training) che può portare a modelli non rappresentativi o ingiusti.

39. In che modo avviene l'"apprendimento" in un Perceptron?
    A. Modificando iterativamente i valori dei pesi (w) e del bias (b) in risposta agli errori di previsione commessi.
    B. Aggiungendo fisicamente nuovi neuroni e connessioni alla rete.
    C. Sostituendo la funzione di attivazione con una più complessa.
    D. Memorizzando staticamente tutte le coppie input-output del dataset.

   **Spiegazione:** Il Supervised Learning richiede un dataset etichettato (con coppie input-output target) per l'addestramento.

40. Da quali strati è composta una *Multi-layer Feed-Forward Neural Network (MFFNN)*?
    A. Esclusivamente da uno strato di input e uno di output, senza intermediari.
    B. Soltanto da una serie di strati nascosti (hidden layers) interconnessi.
    C. Da una sequenza ciclica di nodi che formano un anello chiuso.
    D. Uno strato di Input, uno o più Strati Nascosti (Hidden Layers) e uno strato di Output.

   **Spiegazione:** Una CNN (Convolutional Neural Network) è specializzata ed efficiente per l'analisi di dati a griglia come le immagini.

41. Cos'è l'algoritmo di *Backpropagation*?
    A. Una tecnica statistica per prevedere le tendenze future del mercato azionario.
    B. Il metodo standard per calcolare il gradiente della funzione di errore e aggiornare i pesi della rete procedendo dall'output all'indietro.
    C. Un approccio di clustering per raggruppare neuroni simili.
    D. Un tipo di malware che attacca le reti neurali profonde.

   **Spiegazione:** Backpropagation è l'algoritmo usato per addestrare le reti neurali, propagando l'errore all'indietro per aggiornare i pesi.

42. Quale funzione matematica viene minimizzata durante l'addestramento di una rete neurale?
    A. La funzione di attivazione del singolo neurone.
    B. Il numero totale di neuroni attivi nella rete.
    C. La frequenza di clock della CPU utilizzata per il calcolo.
    D. La Loss Function (Funzione di Perdita o Costo) che misura l'errore del modello.

   **Spiegazione:** Una rete neurale artificiale (ANN) è un modello computazionale ispirato alla struttura dei neuroni biologici.

43. Le *Convolutional Neural Networks (CNN)* eccellono particolarmente in:
    A. Analisi di semplici fogli di calcolo con poche colonne.
    B. Previsione di serie temporali finanziarie basate puramente su dati storici numerici.
    C. Compiti di Computer Vision (immagini) e riconoscimento di pattern locali in dati griglia.
    D. Segmentazione della clientela basata su dati demografici tabellari.

   **Spiegazione:** OLAP (On-line Analytical Processing) supporta analisi multidimensionali complesse e veloci su grandi moli di dati storici.

44. Qual è la funzione degli strati di *Pooling* in una architettura CNN?
    A. Invertire i colori dell'immagine per creare un negativo.
    B. Calcolare la somma pesata degli input per la classificazione.
    C. Ridurre la dimensionalità spaziale (downsampling) delle feature map, rendendo la rappresentazione più compatta e robusta.
    D. Aumentare artificialmente la risoluzione dell'immagine di input.

   **Spiegazione:** Nelle dispense (Lecture 06) si afferma esplicitamente che i "Pooling layers" servono a fare **downsample** (sottocampionamento) dei feature vectors, ottenendo una "smaller representation" (versione più piccola) dei dati. L'opzione A si riferisce ai *Convolutional Layers* (che estraggono le feature), mentre l'opzione D descrive l'opposto del pooling (upsampling). 

45. Per quale tipo di dati sono ideali le *Recurrent Neural Networks (RNN)*?
    A. Immagini statiche ad alta risoluzione.
    B. Dati sequenziali (es. testo, audio, serie storiche) dove l'ordine temporale o posizionale è significativo.
    C. Dataset statici dove i record sono completamente indipendenti l'uno dall'altro.
    D. Compiti di clustering non supervisionato su dati tabellari.

   **Spiegazione:** I dati Nominali (o Categorici) non hanno un ordine intrinseco (es. Colore: Rosso, Blu).

46. Qual è la definizione corretta di *Deep Learning*?
    A. Un metodo mnemonico per apprendere grandi quantità di informazioni testuali.
    B. L'analisi manuale approfondita dei dati da parte di esperti umani.
    C. Un sinonimo esatto dell'algoritmo K-means.
    D. Una sottoclasse del Machine Learning che utilizza reti neurali profonde (molti strati) per apprendere rappresentazioni gerarchiche dei dati.

   **Spiegazione:** Il Deep Learning utilizza reti neurali profonde con molti layer nascosti per apprendere rappresentazioni gerarchiche dei dati.

47. In un neurone artificiale, come viene calcolato l'output $y$ prima dell'applicazione della funzione di attivazione?
    A. $y = x_1 - x_2$ (Differenza degli input)
    B. $y = x_1 \times x_2 \times x_3$ (Prodotto degli input)
    C. $y = \sum (w_i \times x_i) + b$ (Somma pesata degli input più il bias)
    D. $y = \max(x_i)$ (Massimo valore tra gli input)

   **Spiegazione:** La Regressione predice un valore numerico continuo basandosi sulle variabili di input.

48. Cosa può accadere se il *Learning Rate* ($\alpha$) è impostato su un valore troppo alto?
    A. Il modello potrebbe oscillare violentemente attorno al minimo senza mai convergere, o addirittura divergere.
    B. Non succede nulla di rilevante, il training procede normalmente.
    C. Il modello diventa eccessivamente preciso e va in overfitting quasi immediatamente.
    D. Il modello apprende troppo lentamente, richiedendo un tempo eccessivo per convergere.

   **Spiegazione:** L'Overfitting avviene quando un modello impara troppo bene i dettagli (e il rumore) del training set, performando male su nuovi dati.

49. Qual è la principale differenza concettuale tra il *Neurode* (McCulloch-Pitts) e il *Perceptron*?
    A. Il Neurode opera su segnali digitali, mentre il Perceptron è una macchina analogica.
    B. Sono essenzialmente identici e i termini sono sinonimi storici.
    C. Il Neurode possiede pesi adattabili, mentre il Perceptron è fisso.
    D. Il Perceptron introduce pesi e bias *apprendibili*, mentre il Neurode implementava una logica fissa predeterminata.

50. La funzione di attivazione *Softmax* viene tipicamente utilizzata:
    A. Per rimuovere l'influenza del bias dai calcoli.
    B. Per inizializzare tutti i pesi della rete a zero.
    C. Nello strato di output di reti per classificazione multi-classe, per trasformare i valori in una distribuzione di probabilità.
    D. Nello strato di input per normalizzare i dati grezzi.

   **Spiegazione:** La pulizia dei dati (Cleaning) avviene all'inizio del KDP per rimuovere rumore e incongruenze, preparando dati di qualità per l'analisi.

### PARTE 5: TEXT MINING & NLP

51. Qual è lo scopo primario del *Text Mining*?
    A. Estrarre conoscenza implicita, pattern e informazioni di valore da grandi collezioni di testo non strutturato.
    B. Correggere automaticamente gli errori grammaticali e sintattici nei documenti.
    C. Tradurre fedelmente testi da una lingua naturale ad un'altra (Machine Translation).
    D. Sintetizzare vocalmente il testo scritto per l'interazione uomo-macchina.

   **Spiegazione:** L'NLP è la disciplina focalizzata sulla comprensione profonda e generazione del linguaggio (morfologia, sintassi, semantica). Il Text Mining utilizza tecniche di NLP con l'obiettivo primario di estrarre pattern, trend e informazioni strutturate da testi non strutturati. 

52. Come si differenziano *NLP* (Natural Language Processing) e *Text Mining*?
    A. Non esiste alcuna differenza reale, sono termini intercambiabili per la stessa disciplina.
    B. L'NLP si focalizza sulla comprensione e generazione del linguaggio (spesso frase per frase), mentre il TM mira all'estrazione di pattern statistici da grandi corpora.
    C. L'NLP si occupa solo di dati numerici, mentre il TM gestisce le parole.
    D. Il Text Mining è una disciplina molto più antica e obsoleta rispetto al moderno NLP.

   **Spiegazione:** La Stopword Removal nel Text Mining elimina parole molto comuni ma poco informative per ridurre la dimensione del vocabolario.

53. In cosa consiste il processo di *Tokenizzazione*?
    A. Nel segmentare il flusso di testo continuo in unità elementari discrete (token), come parole, numeri o punteggiatura.
    B. Nel rimuovere tutte le parole ritenute inutili o ridondanti.
    C. Nell'identificare il soggetto logico e il verbo principale della frase.
    D. Nel convertire ogni parola nel suo corrispondente codice numerico ASCII.

   **Spiegazione:** La Tokenization è il processo di suddivisione del testo in unità minime chiamate token (solitamente parole o punteggiatura).

54. Cosa sono le *Stop Words* nell'analisi testuale?
    A. Parole chiave di altissima importanza che riassumono il contenuto del documento.
    B. Errori ortografici comuni che devono essere corretti prima dell'analisi.
    C. Parole speciali che segnalano la fine (stop) di un file o di una sezione.
    D. Parole ad alta frequenza (es. "il", "di", "a") che portano scarso contenuto semantico e vengono spesso rimosse.

   **Spiegazione:** Le Stopwords sono parole comuni (es. 'il', 'di', 'a') che spesso vengono rimosse perchè portano poco significato semantico specifico.

55. Qual è la differenza tecnica tra *Stemming* e *Lemmatizzazione*?
    A. Lo Stemming è un processo molto più lento e preciso della Lemmatizzazione.
    B. La Lemmatizzazione funziona solo per la lingua inglese, lo Stemming è universale.
    C. Lo Stemming tronca grezzamente suffissi/prefissi (spesso creando radici non valide), mentre la Lemmatizzazione riduce la parola alla sua forma base corretta (lemma) usando regole morfologiche.
    D. Lo Stemming utilizza un dizionario completo, mentre la Lemmatizzazione si basa su semplici regole euristiche.

   **Spiegazione:** La Lemmatization riduce le parole alla loro forma base (lemma) considerando il contesto morfologico (es. 'andava' -> 'andare').

56. L'algoritmo di *Porter* è un esempio classico di:
    A. Stemmer (Algoritmo di Stemming).
    B. Text Classifier (Classificatore di Testo).
    C. Tokenizer (Algoritmo di segmentazione).
    D. POS Tagger (Part-of-Speech Tagger).

   **Spiegazione:** POS Tagging (Part-of-Speech) assegna a ogni parola la sua categoria grammaticale (es. Nome, Verbo, Aggettivo).

57. A cosa serve il *POS Tagging* (Part-of-Speech Tagging)?
    A. Ad assegnare a ogni parola nel testo la sua corretta categoria grammaticale (es. Nome, Verbo, Aggettivo).
    B. A rimuovere tutta la punteggiatura e i caratteri speciali dal testo.
    C. A trovare sinonimi e contrari per arricchire il vocabolario del testo.
    D. A tradurre la frase in una lingua target mantenendo la struttura sintattica.

   **Spiegazione:** Per una RNN applicata al testo, l'input tipico è una sequenza di vettori, dove ogni vettore rappresenta una parola (Word Embedding).

58. Il sistema *NER* (Named Entity Recognition) è progettato per identificare:
    A. Esclusivamente i verbi di azione e i loro tempi.
    B. La lunghezza media delle frasi e la complessità del testo.
    C. Entità specifiche nel testo e classificarle in categorie come Persone, Organizzazioni, Luoghi, Date.
    D. Errori grammaticali e stilistici nella scrittura.

   **Spiegazione:** NER (Named Entity Recognition) identifica e classifica le entità nominate nel testo (es. Persone, Organizzazioni, Luoghi).

59. Cosa rappresenta un *Dependency Tree* (Albero di Dipendenza)?
    A. Le relazioni grammaticali dirette e funzionali tra le parole (es. chi è il soggetto di quale verbo).
    B. La struttura gerarchica della frase divisa in costituenti sintattici (Sintagma Nominale, Verbale).
    C. La frequenza assoluta di ogni parola all'interno del documento.
    D. L'albero genealogico dell'autore del testo analizzato.

   **Spiegazione:** Controllando testualmente il documento "APPUNTI LEZIONE_mining copia.pdf" a pagina 32 (circa riga 2332-2336 nel file di testo consolidato), la definizione data è esattamente: *"Lessico (Lexicon): Generalmente ha una forma altamente strutturata, immagazzinando i significati e gli usi di ciascuna parola e codificando le relazioni tra parole e significati."* Differisce dalla definizione linguistica standard (che spesso distingue il lessico dalle ontologie relazionali), ma ai fini dell'esame vale quanto scritto negli appunti. 

60. Il modello *Bag of Words* (BoW) rappresenta un documento come:
    A. Un vettore o insieme di frequenze delle parole, ignorando completamente l'ordine e la struttura grammaticale.
    B. Una sequenza ordinata di parole che preserva il contesto sintattico originale.
    C. Un breve riassunto generato automaticamente del contenuto del testo.
    D. Una semplice lista delle parole da escludere (stop words).

   **Spiegazione:** Bag of Words (BoW) rappresenta il testo come un insieme non ordinato di parole, contando la frequenza ma ignorando la grammatica e l'ordine.

61. Cosa indicano *TF* (Term Frequency) e *DF* (Document Frequency)?
    A. La velocità di lettura media richiesta per comprendere il testo.
    B. TF: frequenza del termine nel singolo documento; DF: numero di documenti nel corpus che contengono il termine.
    C. Il numero di errori grammaticali (TF) e sintattici (DF) presenti nel testo.
    D. La lunghezza del documento in parole (TF) e in caratteri (DF).

   **Spiegazione:** IDF (Inverse Document Frequency) diminuisce il peso delle parole che appaiono in troppi documenti del corpus, considerandole meno distictive.

62. Cos'è un *Word Embedding*?
    A. Una tecnica per inserire (embed) celle di testo in fogli di calcolo Excel.
    B. Un particolare tipo di font ottimizzato per la lettura su schermo.
    C. Un algoritmo di compressione per ridurre la dimensione dei file di testo.
    D. Una rappresentazione vettoriale densa delle parole, dove parole semanticamente simili hanno vettori spazialmente vicini.

   **Spiegazione:** Word Embedding (es. Word2Vec) mappa le parole in vettori numerici in uno spazio continuo, catturando relazioni semantiche.

63. Nel contesto della classificazione, cos'è il *Training Set*?
    A. L'insieme di dati non etichettati che il modello deve categorizzare.
    B. Un insieme di dati riservato esclusivamente per la verifica finale delle prestazioni.
    C. Il manuale utente che spiega come utilizzare il software.
    D. L'insieme di dati con etichette note utilizzato per addestrare i parametri del modello.

   **Spiegazione:** Training Set e Test Set: il primo serve per istruire il modello, il secondo per valutarne le prestazioni su dati mai visti.

64. Qual è la caratteristica distintiva del classificatore *Naïve Bayes*?
    A. È basato su reti neurali profonde multistrato.
    B. Non utilizza alcun concetto probabilistico nel suo funzionamento.
    C. Applica il teorema di Bayes assumendo l'indipendenza condizionale "ingenua" (naïve) tra le feature.
    D. È un metodo di clustering non supervisionato e non di classificazione.

   **Spiegazione:** Naive Bayes assume l'indipendenza condizionale tra le feature, semplificando il calcolo della probabilità (spesso efficace per il testo).

65. Come opera una *Support Vector Machine (SVM)* nella classificazione binaria lineare?
    A. Cerca l'iperpiano che separa le due classi massimizzando il margine (distanza dai support vectors).
    B. Genera una serie di regole If-Then casuali fino a trovare una combinazione funzionante.
    C. Raggruppa i dati in cerchi concentrici basati sulla densità.
    D. Calcola semplicemente la media dei punti di ogni classe e usa quella come riferimento.

   **Spiegazione:** La Classificazione è un compito di apprendimento supervisionato dove si prevede l'etichetta di classe discreta per nuovi dati.

66. La *Sentiment Analysis* è classificabile come un compito di:
    A. Machine Translation (Traduzione Automatica).
    B. Data Cleaning e pulizia del testo.
    C. Text Classification (es. classificare un testo come Positivo, Negativo o Neutro).
    D. Text Clustering (raggruppamento senza etichette).

   **Spiegazione:** N-gram è una sequenza contigua di N item (parole) da un testo. Es. Bigramma: 'Data Mining'.

67. Cosa si intende per *Features* nel Text Mining tradizionale?
    A. Le caratteristiche numeriche estratte dal testo (es. presenza di parole, n-grammi) usate come input per gli algoritmi.
    B. I difetti o bug non ancora risolti nel software di analisi.
    C. Le voci del menu di configurazione del programma.
    D. I colori utilizzati nei grafici dei risultati.

   **Spiegazione:** Definire il Text Mining semplicemente come "una tecnica di NLP" è riduttivo ed errato. È un campo multidisciplinare (KDD applicato al testo) che *integra* NLP, statistica e Machine Learning. 

68. Qual è un vantaggio chiave del Deep Learning rispetto al Machine Learning classico per il testo?
    A. Richiede dataset di addestramento molto più piccoli.
    B. I modelli sono molto più semplici da interpretare e spiegare (White Box).
    C. Funziona in modo efficiente anche su hardware obsoleto e CPU lente.
    D. Esegue il *Feature Learning* automatico, eliminando la necessità di una complessa ingegnerizzazione manuale delle feature.

   **Spiegazione:** L'Epoch nel Deep Learning è un passaggio completo dell'intero training set attraverso la rete neurale.

69. Se un compito di classificazione prevede output come "Sport", "Politica", "Tecnologia", si tratta di:
    A. Regressione Lineare.
    B. Classificazione Binaria (Binary Classification).
    C. Clustering Non Supervisionato.
    D. Classificazione Multi-classe (Multi-class Classification).

   **Spiegazione:** Se si dispone di un Training Set (dati etichettati), l'approccio corretto è la Classificazione (Supervised Learning).

70. Come si distingue nel testo la parola "Apple" (Azienda) da "Apple" (Frutta)?
    A. Convertendo tutto il testo in minuscolo (Lowercasing).
    B. Applicando lo Stemming (riducendo entrambe a "Appl").
    C. Rimuovendo le Stop Words circostanti.
    D. Analizzando il contesto semantico e usando il Named Entity Recognition (NER) o Embeddings contestuali.

   **Spiegazione:** Lo Stemming tronca le parole alla loro radice (stem) in modo euristico, spesso rimuovendo suffissi, senza garantire che la radice sia una parola valida.

### PARTE 6: MISTI E SCENARI APPLICATIVI

71. Un'azienda vuole segmentare i clienti in gruppi comportamentali simili senza categorie predefinite. Quale tecnica userà?
    A. Query SQL di selezione semplice.
    B. Classificazione Supervisionata.
    C. Clustering (Analisi dei Gruppi).
    D. Regressione Lineare.

   **Spiegazione:** SQL (Structured Query Language) è il linguaggio standard per interagire con i database relazionali.

72. Se l'obiettivo è prevedere il valore esatto del fatturato del prossimo mese (un numero continuo), si userà:
    A. Association Rules (Regole di Associazione).
    B. Clustering.
    C. Classificazione.
    D. Regressione.

   **Spiegazione:** Supporto e Confidenza sono le metriche chiave per valutare la qualità delle regole di associazione.

73. La "Market Basket Analysis", che identifica quali prodotti vengono acquistati insieme, è un esempio di:
    A. Association Analysis / Pattern Mining.
    B. Outlier Detection.
    C. Classificazione Supervisionata.
    D. Text Mining.

   **Spiegazione:** Outlier Analysis è il processo di identificazione di dati che si discostano significativamente dalla norma.

74. Quali metriche valutano la qualità di una regola di associazione?
    A. Mean e Variance (Media e Varianza).
    B. Support (frequenza) e Confidence (affidabilità).
    C. K (numero di cluster) e Epsilon (raggio).
    D. Loss Function e Accuracy.

75. Qual è la sintassi SQL corretta per ordinare i risultati dal più alto al più basso?
    A. SORT DOWN
    B. ORDER BY column_name DESC
    C. GROUP BY column_name
    D. ORDER BY column_name ASC

76. Qual è la complessità temporale dell'algoritmo *K-means*?
    A. Esponenziale rispetto al numero di dati.
    B. Lineare ($O(n \cdot k \cdot d \cdot i)$) dove n è il numero di oggetti, rendendolo efficiente.
    C. Cubica ($O(n^3)$), quindi molto lento.
    D. Costante ($O(1)$), istantaneo.

   **Spiegazione:** Il Cross-Validation (es. K-Fold) serve a stimare l'affidabilità del modello dividendo i dati in K parti e ruotando training/test.

77. Qual è la complessità temporale tipica del *Clustering Gerarchico* standard?
    A. $O(n^3)$ o $O(n^2 \log n)$, rendendolo computazionalmente oneroso per grandi dataset.
    B. Lineare $O(n)$, molto veloce.
    C. Costante $O(1)$.
    D. $O(k)$, dipendente solo dal numero di cluster.

78. Il comando SQL `INSERT INTO` viene utilizzato per:
    A. Aggiungere nuovi record (righe) in una tabella esistente.
    B. Modificare i valori di record già presenti.
    C. Rimuovere record dalla tabella.
    D. Definire la struttura di una nuova tabella.

79. Cosa si intende per *Outlier* in un dataset?
    A. Un punto dati che si discosta significativamente dagli altri, indicando un'anomalia o un errore.
    B. Il valore medio esatto della distribuzione.
    C. Un dato perfettamente rappresentativo della norma.
    D. Un cluster con una densità di punti molto elevata.

   **Spiegazione:** Un Outlier è un oggetto dato che si discosta significativamente dal resto dei dati, come se fosse stato generato da un meccanismo diverso.

80. Cosa significa che una matrice Term-Document è *Sparsa*?
    A. Che è composta quasi interamente da numeri 1.
    B. Che contiene dati errati o corrotti.
    C. Che la stragrande maggioranza dei valori è zero, poiché ogni documento contiene solo una piccola frazione del vocabolario totale.
    D. Che la matrice ha dimensioni molto ridotte.

   **Spiegazione:** Una matrice è sparsa ('Sparse') quando la maggior parte dei suoi elementi è zero. In una matrice Term-Document, la maggior parte delle parole del dizionario non appare in ogni singolo documento.

81. Quale architettura di rete neurale utilizza filtri (kernel) per estrarre feature locali?
    A. Convolutional Neural Network (CNN).
    B. K-means Clustering.
    C. Recurrent Neural Network (RNN).
    D. Multilayer Perceptron (MLP) fully connected.

   **Spiegazione:** Le CNN (Convolutional Neural Networks) utilizzano livelli di convoluzione con filtri (kernel) per catturare pattern locali e spaziali nei dati (es. immagini).

82. Il parametro *Learning Rate* controlla:
    A. La grandezza dell'aggiustamento dei pesi ad ogni passo dell'addestramento.
    B. La dimensione totale del dataset di input.
    C. La durata temporale totale del processo di training.
    D. Il numero di neuroni presenti in ogni strato.

   **Spiegazione:** Il Learning Rate determina la dimensione del passo di aggiornamento dei pesi durante l'ottimizzazione; troppo alto o basso causa problemi.

83. A cosa serve la *Confusion Matrix* in un problema di classificazione?
    A. A fornire una visione dettagliata delle performance del modello (Veri Positivi, Falsi Positivi, ecc.).
    B. A confondere intenzionalmente l'utente per sicurezza.
    C. A calcolare la distanza euclidea tra i cluster trovati.
    D. A riorganizzare fisicamente i dati nel database.

   **Spiegazione:** La Confusion Matrix è una tabella usata per valutare le performance di un classificatore (Veri Positivi, Falsi Positivi, ecc).

84. Quale dei seguenti è il miglior esempio di dati *non strutturati*?
    A. Una tabella "Dipendenti" in un database SQL.
    B. Un foglio Excel con colonne "Data", "Importo", "Venditore".
    C. Il corpo del testo di un'email o un post sui social media.
    D. Un file CSV (Comma Separated Values) ben formattato.

   **Spiegazione:** I dati non strutturati non seguono uno schema predefinito. Esempi includono testo libero (email, social post), immagini, audio e video.

85. Le operazioni di *Data Transformation* includono tipicamente:
    A. L'acquisto di nuovo hardware per l'elaborazione.
    B. La normalizzazione, l'aggregazione e la generalizzazione dei dati.
    C. La cancellazione fisica dei file di database.
    D. La stesura del report finale per gli stakeholder.

   **Spiegazione:** La Data Transformation include la normalizzazione, l'aggregazione, e la creazione di nuovi attributi per rendere i dati adatti al Data Mining.

86. Nel clustering partizionale standard (Hard Clustering), un oggetto appartiene a:
    A. Nessun cluster (rimane non assegnato).
    B. Esattamente ed esclusivamente un solo cluster.
    C. Più cluster contemporaneamente con diversi gradi di appartenenza.
    D. Tutti i cluster contemporaneamente.

87. I *Dendriti* del neurone biologico hanno la funzione analoga a quale componente nel Perceptron?
    A. Alla funzione di attivazione.
    B. Al termine di Bias.
    C. All'output finale.
    D. Agli input e alle loro connessioni pesate.

88. L'*Assone* del neurone biologico corrisponde a:
    A. I pesi sinaptici.
    B. Il corpo cellulare (Soma).
    C. L'uscita (output) del segnale verso altri neuroni.
    D. I segnali di ingresso.

89. Qual è lo scopo di utilizzare un *Validation Set* separato?
    A. Assegnare il voto finale agli studenti.
    B. Eseguire l'addestramento principale dei pesi (backpropagation).
    C. È un passaggio ridondante e inutile.
    D. Monitorare le performance durante il training per calibrare gli iperparametri ed evitare l'overfitting.

90. Il comando `CREATE TABLE` fa parte di quale sotto-linguaggio SQL?
    A. DDL (Data Definition Language).
    B. DML (Data Manipulation Language).
    C. DCL (Data Control Language).
    D. HTML (HyperText Markup Language).

91. I *Word Clouds* sono utilizzati principalmente per:
    A. La presentazione visiva (Knowledge Presentation) dei termini più frequenti in un testo.
    B. La pulizia automatica dei dati (Data Cleaning).
    C. La gestione delle transazioni nel database.
    D. L'ottimizzazione degli algoritmi genetici.

   **Spiegazione:** I Word Clouds offrono una rappresentazione visiva immediata dove la dimensione delle parole è proporzionale alla loro frequenza nel corpus. 

92. Dividere automaticamente notizie finanziarie in categorie ("Tech", "Pharma") usando esempi etichettati è un compito di:
    A. Text Classification (Supervised Learning).
    B. Clustering Non Supervisionato.
    C. Data Integration.
    D. Association Rules Mining.

   **Spiegazione:** Il Data Mining è interdisciplinare: unisce Statistica, Database, AI/Machine Learning e Visualizzazione.

93. Rispetto a un semplice modello unigramma, i *N-grammi* (es. bigrammi) offrono il vantaggio di:
    A. Eliminare automaticamente tutte le stop words.
    B. Ridurre drasticamente la dimensione del vocabolario.
    C. Catturare parzialmente il contesto locale e l'ordine delle parole (es. "New York").
    D. Essere computazionalmente più leggeri da calcolare.

   **Spiegazione:** I N-grammi (come i bigrammi) catturano l'ordine locale delle parole e il contesto (es. distinzione tra "cane morde uomo" e "uomo morde cane").

94. Il fenomeno dell'*Overfitting* si verifica quando:
    A. Il computer si surriscalda per il troppo lavoro.
    B. Il modello impara eccessivamente i dettagli e il rumore del training set, perdendo la capacità di generalizzare su nuovi dati.
    C. Il modello è perfetto e non commette errori neanche sul test set.
    D. Il modello è troppo semplice per catturare la complessità dei dati (underfitting).

   **Spiegazione:** L'Overfitting (sovradattamento) accade quando il modello impara 'a memoria' i dati di training, incluso il rumore, fallendo nella generalizzazione su nuovi dati.  

95. La caratteristica *Non-volatile* di un Data Warehouse implica che:
    A. I dati vengono cancellati e sovrascritti ogni notte.
    B. I dati sono temporanei e volatili come nella RAM.
    C. I dati storici, una volta caricati, non vengono modificati ma solo consultati per analisi.
    D. I dati sono fisicamente instabili e a rischio perdita.

   **Spiegazione:** 'Non-volatile' significa che i dati nel Data Warehouse sono storici e stabili: una volta caricati, non vengono modificati o cancellati come nei database transazionali.

96. Per trovare la linea di tendenza (trend) in un grafico a dispersione si usa:
    A. La Regressione Lineare.
    B. L'algoritmo K-means.
    C. Un Istogramma delle frequenze.
    D. Il Clustering Gerarchico.

   **Spiegazione:** Per analizzare trend continui nel tempo (es. interesse d'acquisto), la Regressione è la tecnica statistica più indicata.

97. *Sinonimia* e *Polisemia* sono sfide tipiche in quale ambito?
    A. Analisi numerica di dati finanziari.
    B. NLP e Text Mining (dovute all'ambiguità intrinseca del linguaggio naturale).
    C. Elaborazione di immagini satellitari.
    D. Progettazione di database relazionali normalizzati.

   **Spiegazione:** Sinonimia e Polisemia sono ambiguità intrinseche del linguaggio naturale che rendono complesse le task di Text Mining e NLP.

98. Nel Text Mining, cos'è un *Corpus*?
    A. La struttura fisica del neurone artificiale.
    B. Una parte anatomica umana.
    C. Una vasta e strutturata collezione di documenti testuali usata per l'analisi.
    D. Un errore di run-time nel codice Python.

   **Spiegazione:** Il Corpus è l'intera collezione di documenti testuali oggetto di analisi.

99. Qual è l'input tipico per una rete *RNN* applicata al testo?
    A. Una sequenza ordinata di vettori di parole (Word Vectors).
    B. Una singola immagine statica.
    C. Una query SQL complessa.
    D. Un singolo valore scalare.

100. Il *Knowledge Discovery* (KDD) è descritto come un processo:
     A. Impossibile da realizzare con le tecnologie attuali.
     B. Banale, istantaneo e privo di complessità.
     C. Completamente automatico, senza alcuna necessità di supervisione umana.
     D. Iterativo, interattivo e composto da più fasi raffinate progressivamente.

   **Spiegazione:** Il KDD è descritto come un processo *iterativo e interattivo*, non lineare e automatico al 100%, richiedendo spesso il feedback umano.

### PARTE INTEGRATIVA: NUOVE DOMANDE

101. Cos'è CBOW?
   A. CBOW sta per Continuous Bag of Words è una tecnica che permette di effettuare la tokenization
   B. Nessuna delle opzioni rappresenta una corretta definizione di CBOW
   C. CBOW sta per Continuous Bag of Words è una tecnica che permette di effettuare la Named Entity Recognition (NER)
   D. CBOW sta per Continuous Bag of Words è una delle architetture su cui si basa la tecnica Word2Vec

   **Spiegazione:** Il testo conferma che CBOW (Continuous Bag of Words) è una delle due architetture principali utilizzate dalla tecnica **word2vec** (l'altra è skip-gram).

102. A cosa si fa riferimento quando si legge la seguente definizione: "… si riferisce ai valori veri e verificati o alle etichette utilizzate come benchmark per l'addestramento e la valutazione di modelli."?
   A. clustering a partizionamento
   B. ground-truth
   C. clustering gerarchico
   D. falsi positivi, falsi negativi

   **Spiegazione:** La definizione nella domanda corrisponde esattamente alla descrizione di **Ground Truth** (verità di base) fornita nelle slide. Si riferisce ai valori o alle etichette verificate (etichette "label") usate come riferimento standard per addestrare e valutare i modelli.

103. Dati e Noise. Qual è l'obiettivo di una funzione di "image denoising"? Scegli la risposta che ritieni più opportuna.
   A. Nessuna delle opzioni è corretta
   B. Eliminare il cosiddetto "noise" e il "bias" a livello dei pixel.
   C. Eliminare il cosiddetto "noise" a livello dei pixel
   D. Eliminare il cosiddetto "bias" a livello dei pixel

104. Quale tecnica di text mining fornisce un output del genere? "Tim Cook" - PERSON (People, including fictional) "Microsoft" - ORG (Companies, agencies, institutions) "Seattle" - GPE (Countries, cities, states) "Friday" - DATE (Absolute or relative dates or periods) "AI" - ORG (Companies, agencies, institutions) "$50 million" - MONEY
   A. POS Tagging
   B. Tokenization
   C. Nessuna delle opzioni
   D. NER

   **Spiegazione:** L'esempio fornito nella domanda è identico a quello presente nelle dispense per spiegare il **NER (Named Entity Recognition)**. Questa tecnica identifica e classifica entità come persone, organizzazioni, luoghi, date e valori monetari all'interno del testo.
Il *POS Tagging* identifica le parti del discorso (verbi, nomi, ecc.), mentre la *Tokenization* divide il testo in parole/unità. L'output mostrato è chiaramente NER.

105. Quale delle seguenti opzioni rappresenta una corretta definizione di Lexicon?
   A. Archivia la sintassi e gli usi di ogni frase di un corpus testuale
   B. Nessuna delle opzioni fornite è corretta
   C. Archivia i significati e gli usi di ogni parola. Codifica le relazioni tra parole e significati.
   D. Archivia i significati e gli usi di ogni parola, ma non codifica le relazioni tra le parole e i loro significati.

   **Spiegazione:** Hai ragione! Controllando testualmente il documento "APPUNTI LEZIONE_mining copia.pdf" a pagina 32 (circa riga 2332-2336 nel file di testo consolidato), la definizione data è esattamente:
*"Lessico (Lexicon): Generalmente ha una forma altamente strutturata, immagazzinando i significati e gli usi di ciascuna parola e codificando le relazioni tra parole e significati."*
Differisce dalla definizione linguistica standard (che spesso distingue il lessico dalle ontologie relazionali), ma ai fini dell'esame vale quanto scritto negli appunti.

106. A cosa si fa riferimento quando si legge la seguente definizione? "… contiene un sottoinsieme dei dati aziendali complessivi di valore per uno specifico gruppo di utenti, come quelli appartenenti a un reparto aziendale. L’ambito è limitato a soggetti specifici."
   A. Data Mart
   B. Nessuna delle opzioni fornite è corretta
   C. Data Lake
   D. Enterprise Data Warehouse

   **Spiegazione:** La definizione data nella domanda è copiata quasi parola per parola dalla sezione degli appunti che definisce il **Data Mart**. Un Data Mart è descritto proprio come un sottoinsieme del Data Warehouse focalizzato su uno specifico dipartimento o gruppo di utenti (es. marketing).
L'Enterprise Data Warehouse (Option A) raccoglie invece i dati dell'*intera* organizzazione.
Il Data Lake (Option (C) raccoglie dati *grezzi* di varia natura.

107. Il dendrogramma è una rappresentazione grafica che consente di analizzare e leggere informazioni relative a ... (scegli l'opzione corretta).
   A. K-means
   B. DBSCAN
   C. Clustering gerarchico
   D. Nessuna delle opzioni fornite è corretta

108. Quale degli step di Text Mining è caratterizzato dalla seguente proprietà? "… è un approccio statistico che assegna una probabilità di argomento a ogni parola."
   A. Topic Modeling
   B. NER (Named Entity Recognition)
   C. Tokenization
   D. Lemming

   **Spiegazione:** La definizione nella domanda proviene direttamente dal paragrafo degli appunti relativo al **Topic Model** (o Topic Modeling). Lì si afferma che è un "approccio statistico che assegna un valore di probabilità di argomento a ciascuna parola", permettendo di capire di cosa parla un documento analizzando le parole che lo compongono.

109. Quale delle seguenti affermazioni relative al concetto di pattern è corretta?
   A. I pattern possono essere trovati solo nei dati non strutturati
   B. Nessuna delle opzioni fornite è corretta
   C. I pattern possono essere trovati sia nei dati strutturati che nei dati non strutturati
   D. I pattern possono essere trovati solo nei dati strutturati

110. Quale delle seguenti affermazioni è corretta?
   A. Il criterio di convergenza del perceptron non dipende dalla separabilità lineare dei dati.
   B. Il criterio di convergenza del perceptron dipende dal numero di variabili di input
   C. Il criterio di convergenza del perceptron dipende esclusivamente dal valore dei pesi assegnati
   D. Il criterio di convergenza del perceptron dipende dalla separabilità lineare dei dati.

111. Qual è il ruolo dei Pooling Layer nelle CNNs (Convolutional Neural Networks)?
   A. I layer di pooling permettono di restituire una versione più grande del dato che viene elaborato in input (sovracampionamento)
   B. I layer di pooling permettono di fornire una versione più piccola del dato che viene elaborato (sottocampionamento)
   C. I layer di pooling permettono di estrarre caratteristiche a partire dai filtri di convoluzione.
   D. I layer di pooling rappresentano l'output layer delle CNNs

112. Quanti step sono inclusi nel cosiddetto KDP (Knowledge Discovery Process)?
   A. 4
   B. 5
   C. 7
   D. 3

   **Spiegazione:** Nel documento "APPUNTI LEZIONE", nella sezione "La Scoperta di Conoscenza come Processo", viene affermato quasi testualmente: *"The Knowledge Discovery Process comprises 7 steps"*.
I passaggi elencati includono tipicamente:
1.  Data Cleaning
2.  Data Integration
3.  Data Selection
4.  Data Transformation
5.  Data Mining
6.  Pattern Evaluation
7.  Knowledge Presentation

(Nota: Sebbene alcuni framework ne contino 5, il testo del corso ne specifica espressamente **7**).

113. Quando si tratta di Multilayer Feedforward Neural Network, quale delle seguenti affermazioni NON è corretta?
   A. Si tratta di reti neurali in cui l'elaborazione dell'informazione prevede l'uso di input layer, layer intermedi (hidden) e output layer.
   B. Si tratta di reti neurali in cui l'elaborazione dell'informazione si propaga in avanti.
   C. Sono reti neurali in cui si utilizzano le cosiddette funzioni di attivazione per fornire l'output di ogni singolo "nodo".
   D. Si tratta di reti neurali in cui l'elaborazione dell'informazione si propaga all'indietro.

   **Spiegazione:** Il termine "Feedforward" significa letteralmente "che alimenta in avanti". In queste reti, l'informazione viaggia solo in una direzione: dall'input, attraverso i layer nascosti (hidden), fino all'output. Non ci sono cicli o loop.
L'opzione B afferma che l'informazione si propaga *all'indietro*, il che è falso per la fase di elaborazione (inference). La propagazione all'indietro (backpropagation) avviene solo durante l'addestramento per aggiornare i pesi, ma non definisce il flusso dell'informazione nella rete feedforward stessa. Essendo una domanda che chiede cosa NON è corretto, la B è la risposta giusta.

114. Un file HTML è un esempio di ... (completa la frase utilizzando una delle opzioni a disposizione).
   A. dato semistrutturato
   B. dato non strutturato
   C. dato strutturato
   D. Nessuna delle opzioni è corretta

115. Quando si tratta di algoritmi di clustering divisivo e agglomerativo, si fa riferimento a ... (scegli l'opzione corretta).
   A. Clustering a partizionamento
   B. DBSCAN
   C. Clustering gerarchico
   D. K-means

116. Si considerino le seguenti tre parole: "gattone", "pescatore", "giocare". Qual è il risultato della lemmatizzazione?
   A. gattone → gattone, pescatore → pescatore, giocare → giocare
   B. gattone → gatto, pescatore → pescatore, giocare → giocare
   C. gattone → gatto, pescatore → pescare, giocare → giocare
   D. gattone → gattone, pescatore → pesca, giocare → gioco

117. La Word Vector Representation è una tecnica ampiamente utilizzata nel text mining. Quale delle seguenti opzioni correttamente rappresenta una sfida per la tecnica menzionata?
   A. La connessione della semantica lessicale delle parole con la sintassi di frasi, enunciati, paragrafi e documenti
   B. La connessione della semantica lessicale delle parole con la semantica di frasi, enunciati, paragrafi e documenti
   C. La connessione della sintassi con la semantica di frasi, enunciati, paragrafi e documenti
   D. Nessuna delle opzioni rappresenta una sfida per la Word Vector Representation.

118. Quale delle seguenti affermazioni relative ai sistemi di clustering a partizionamento NON è corretta?
   A. I sistemi di clustering a partizionamento devono conoscere il numero di cluster di output in anticipo
   B. Tutte le opzioni fornite sono corrette
   C. I sistemi di clustering a partizionamento possono modificare il numero di cluster di output durante l'esecuzione.
   D. I sistemi di clustering a partizionamento suddividono i data points in gruppi esclusivi (i dati appartenenti al cluster A non possono appartenere al cluster B).

   **Spiegazione:** La caratteristica fondamentale (e limitazione) dei metodi di clustering a partizionamento standard come il **K-Mmeans**, come riportato negli appunti, è che *"richiede obbligatoriamente la specifica del numero k di cluster in anticipo"* (Opzione B è vera).
Inoltre, creano partizioni esclusive (un punto sta in un solo cluster, Opzione C è vera).
Di conseguenza, l'affermazione che **NON è corretta** è la A: questi sistemi *non* modificano dinamicamente il numero di cluster durante l'esecuzione; il numero *k* è fissato all'inizio.

119. A cosa si fa riferimento quando si legge la seguente definizione? "... è l’estrazione automatizzata o facilitata di modelli che rappresentano conoscenza implicitamente memorizzata o acquisita in grandi basi di dati, data warehouse, il Web, altri grandi archivi di informazioni o flussi di dati."
   A. Knowledge Discovery Process
   B. Pattern Extraction
   C. Nessuna delle opzioni fornite è corretta
   D. Knowledge Discovery from Data

   **Spiegazione:** La frase riportata nella domanda è la prima parte della **Definizione formale di Data Mining** presente nella sezione introduttiva degli appunti, che recita: *"Il data mining, comunemente noto anche come Knowledge Discovery from Data (KDD), è l’estrazione automatizzata o facilitata di pattern che rappresentano conoscenza implicitamente memorizzata o catturata in grandi database..."*.
Il termine *Knowledge Discovery from Data* (KDD) è usato come sinonimo formale di Data Mining.
Il *Knowledge Discovery Process* (Opzione B) si riferisce tipicamente all'intero ciclo di 7 step (che include anche Data Cleaning, ecc.), ma la definizione testuale fornita coincide con l'etichetta KDD/Data Mining.

120. Cosa indicano i termini w(i) nell'architettura del perceptron?
   A. I valori di input
   B. I pesi (o coefficienti) e il bias
   C. I pesi (o coefficienti)
   D. il bias

121. Completa la seguente affermazione: "Un Data Warehouse è..."
   A. Un gruppo di database
   B. Nessuna delle precedenti
   C. Un sistema che gestisce dati provenienti da diverse fonti (A system that handles data coming from different sources)
   D. Un DBMS

   **Spiegazione:** Un DBMS è il software che gestisce la creazione, l'aggiornamento e l'interrogazione dei database, garantendo integrità e sicurezza.

122. Knowledge Discovery Process (KDP). Quale delle seguenti opzioni include i primi quattro passaggi del KDP?
   A. Data Cleaning, Data Integration, Data Selection, Data Transformation
   B. Nessuna delle precedenti
   C. Data Cleaning, Data Integration, Data Selection, Pattern Evaluation
   D. Knowledge Representation, Data Mining, Pattern Evaluation, Data Cleaning

   **Spiegazione:** Knowledge Representation è la fase finale in cui la conoscenza scoperta viene presentata all'utente (grafici, regole, report).

123. Quale delle seguenti affermazioni è corretta?
   A. Il Data Mining entra in gioco per estrarre pattern che non possono essere facilmente trovati con statistiche e trend
   B. Il Data Mining viene eseguito prima di estrarre statistiche e trend sui dati
   C. Statistiche e trend ci permettono di trovare tutti i pattern nei dati
   D. Nessuna delle precedenti

124. Cosa significa l'acronimo KDD?
   A. Knowledge Database Discovery
   B. Knowledge Discovery Data
   C. Nessuna delle precedenti (Knowledge Discovery from Data)
   D. Knowledge Detection in Databases

125. Schema Relazionale. Seleziona la risposta che rappresenta meglio le proprietà di un database relazionale.
   A. Structured data; Tables; Primary Key
   B. Tables (Tabelle/Entità)
   C. Unstructured data; Tables; Primary Key
   D. Structured data; Tables; Primary Key; External Key

   **Spiegazione:** Le Chiavi Esterne (External/Foreign Keys) sono fondamentali nel modello relazionale per stabilire relazioni tra tabelle e garantire l'integrità referenziale, rendendo l'opzione D più completa rispetto alla A.

126. Quale delle seguenti parole useresti per completare la frase: "Un file foglio di calcolo (spreadsheet) contiene dati _______."
   A. semi-structured (semi-strutturati)
   B. nessuna delle precedenti
   C. structured (strutturati)
   D. unstructured (non strutturati)

127. Quale delle seguenti opzioni si trova nella posizione più in alto nella gerarchia della gestione dell'analisi dei dati?
   A. Data Mining
   B. Database
   C. Data Warehose
   D. Data Cleaning

128. Scenario Data Warehouse. Vuoi analizzare i dati su attributi specifici (es. zoom-in). A quale delle seguenti operazioni OLAP corrisponde?
   A. OLAP (On-Line Analytical Processing)
   B. Nessuna delle precedenti
   C. Roll-up
   D. Drill-down

   **Spiegazione:** OLAP (On-Line Analytical Processing) è la tecnologia per l'analisi multidimensionale interattiva dei dati (cubi OLAP).

129. Quando i risultati possono essere considerati statisticamente significativi?
   A. Nessuna delle precedenti
   B. Quando non superano un test di ipotesi statistica
   C. Quando si riferiscono a database di grandi dimensioni
   D. Quando superano un test di ipotesi statistica (statistically significant hypothesis test)

   **Spiegazione:** I risultati sono statisticamente significativi se superano un test di ipotesi (p-value < soglia), indicando che non sono dovuti al caso.

130. Un analista dati usa OLAP per analizzare dati multidimensionali a diversi livelli di granularità. Cosa significa l'acronimo OLAP?
   A. On-line Analytical Preprocessing
   B. On-line Analytical Processing
   C. Nessuna delle precedenti
   D. On-line Analytical Postprocessing

131. Hai dei dati non categorizzati (uncategorised data). Come puoi generare delle etichette (labels) a partire da essi?
   A. Puoi generare etichette eseguendo una classificazione
   B. Puoi generare etichette usando la regressione
   C. Puoi generare etichette usando l'analisi dei cluster (Cluster Analysis)
   D. Nessuna delle precedenti

132. Ti è stato chiesto di fornire trend sull'interesse dei clienti nell'acquisto di un nuovo articolo in estate. Quale dei seguenti approcci si adatta meglio allo scenario?
   A. Cluster Analysis
   B. Classification
   C. Regression (Regressione)
   D. Nessuna delle precedenti

133. Se hai un training set disponibile, quale dei seguenti approcci può essere eseguito?
   A. Classification (Classificazione)
   B. Nessuna delle precedenti
   C. Regression (Regressione)
   D. Cluster Analysis

134. Quale delle seguenti rappresenta la maggior quantità di tipi di dati sul Web?
   A. Semi-structured Data
   B. Unstructured Data (Dati non strutturati)
   C. Structured Data
   D. None of the above

135. Quale dei seguenti è un acronimo che rappresenta il Data Mining da una prospettiva formale?
   A. KDD (Knowledge Discovery in Databases)
   B. KDP
   C. DM
   D. All of the above (Tutte le precedenti)

136. Dato: Buys(X,Laptop) => [Buys(X,'wireless keyboard'), support=3%, confidence=45%]. Quale delle seguenti affermazioni è corretta?
   A. Il 97% di tutte le transazioni include sia tastiera wireless che laptop
   B. Il 3% di tutte le transazioni include sia tastiera wireless che laptop
   C. C'è una probabilità del 45% che i clienti che acquistano una tastiera comprino un laptop
   D. Nessuna delle precedenti

   **Spiegazione:** Il Teorema di convergenza del Perceptron (e quanto riportato specificamente in *Lecture 03*) stabilisce che l'algoritmo **garantisce la convergenza** (ossia di trovare un iperpiano di separazione e smettere di aggiornare i pesi) **solo se e quando** i dati di input sono **linearmente separabili**. Se i dati non lo sono, (senza accorgimenti come il limite di epoche) l'algoritmo continuerebbe a ciclare all'infinito cercando una soluzione che non esiste.

137. Stai lavorando con un DB transazionale che raccoglie informazioni sugli acquisti: (Id_transaction, amount, date, article_category, id_user) e (Id_category, promotional_offer, discount). Se sei interessato a eseguire una query per estrarre gli sconti per un singolo utente, con quale sezione IT stai lavorando?
   A. Data Warehouse
   B. DBMS (Data Base Management System)
   C. Data Lake
   D. Data Mart

138. Quale delle seguenti affermazioni NON si applica al LLM Foundation Model?
   A. È un modello linguistico ampio e pre-addestrato utilizzabile per vari compiti di NLP.
   B. È addestrato su enormi quantità di dati testuali e può essere usato per generare testo.
   C. È sottoposto a fine-tuning per il dialogo conversazionale.
   D. Nessuna delle precedenti.

   **Spiegazione:** I Foundation Models (es. GPT-4 base) sono modelli generici pre-addestrati. Il fine-tuning specifico per il dialogo (Chat) è ciò che caratterizza prodotti come ChatGPT, distinguendoli dal modello base. 

139. Text Mining e NLP differiscono? In quali termini? Scegli l'affermazione corretta.
   A. L'NLP mira a comprendere il linguaggio umano mentre il Text Mining si occupa di Information Extraction (estrazione di informazioni).
   B. Il Text Mining mira a comprendere il linguaggio umano mentre l'NLP si occupa di Information Extraction.
   C. Il Text Mining si concentra sulla sintassi.
   D. Nessuna delle precedenti.

140. Una delle seguenti affermazioni sul Text Mining è errata. Quale?
   A. Il Text Mining è una tecnologia integrata di NLP, Pattern Classification e Machine Learning.
   B. Il Text Mining è una tecnica di NLP.
   C. I compiti di Text Mining non sono solitamente lineari poiché l'informazione è concepita nel testo.
   D. Discovery, Induction e Refinement sono compiti tipici del Text Mining.

141. Un file HTML appartiene a quale delle seguenti categorie?
   A. Nessuna delle precedenti.
   B. File Semi-strutturato.
   C. File Strutturato.
   D. File Non strutturato.

142. Quale dei seguenti è menzionato come punto debole (weakness) del Foundation LLM?
   A. Limited Scope (Ambito Limitato).
   B. Bias.
   C. Computational Cost (Costo Computazionale).
   D. Nessuna delle precedenti.

### PARTE SPECIALE: RAGIONAMENTO E LOGICA APPLICATA

143. Stai analizzando un dataset di transazioni bancarie per rilevare frodi. Le frodi sono rarissime (0.1%). Addestri un classificatore che ottiene un'Accuracy del 99.9%. Tuttavia, il sistema non rileva nessuna frode. Qual è il problema logico?
   A. Hai usato un Learning Rate troppo alto, causando overfitting sulla classe minoritaria.
   B. Il modello ha bisogno di più dati di "Non Frode" per capire meglio la normalità.
   C. In caso di classi sbilanciate, l'Accuracy è fuorviante (Accuracy Paradox). Il modello sta semplicemente predicendo sempre la classe maggioritaria ("Non Frode").
   D. L'Accuracy è la metrica perfetta, quindi il modello è ottimo; probabilmente non ci sono frodi nel test set.

   **Spiegazione:** Quando le classi sono sbilanciate (es. 99.9% vs 0.1%), un modello che predice sempre la classe maggioritaria avrà un'accuracy altissima (99.9%) ma sarà inutile (Recall sulle frodi = 0%). Bisogna usare Precision, Recall o F1-Score.

144. Un tuo collega propone di usare K-Means per clusterizzare un dataset che contiene cluster di forma allungata e irregolare (non sferica) e molto noise. Perché questa scelta è logicamente sbagliata?
   A. K-Means assume cluster sferici (minimizza la varianza) e soffre molto gli outlier, quindi fallirà nel separare forme complesse. DBSCAN sarebbe più indicato.
   B. K-Means è ottimo per forme irregolari ma lento.
   C. K-Means funziona solo su dati categorici, mentre le forme implicano coordinate numeriche.
   D. Non è sbagliata, basta impostare K molto alto.

   **Spiegazione:** K-Means cerca cluster convessi (sferici/globulari). Se i dati hanno cluster non sferici o densità variabile, K-Means "spacca" i cluster naturali in modo errato. DBSCAN segue la densità e gestisce forme arbitrarie.

145. Hai una tabella SQL `Ordini` e una `Clienti`. Vuoi trovare tutti i Clienti che NON hanno mai effettuato un ordine. Quale logica di JOIN useresti?
   A. INNER JOIN tra Clienti e Ordini.
   B. CROSS JOIN per vedere tutte le combinazioni.
   C. RIGHT JOIN da Clienti a Ordini.
   D. LEFT JOIN da Clienti a Ordini, filtrando DOVE l'ID Ordine IS NULL.

   **Spiegazione:** Un LEFT JOIN prende tutti i Clienti. Se non c'è corrispondenza in Ordini, i campi dell'Ordine saranno NULL. Filtrando per `Ordine.ID IS NULL` isoli chi non ha comprato.

146. Stai addestrando una Rete Neurale e noti che l'errore sul Training Set scende costantemente, ma l'errore sul Validation Set inizia a salire dopo l'epoca 50. Cosa sta succedendo e come intervieni?
   A. Underfitting. Devi aggiungere più neuroni.
   B. Overfitting. Il modello sta memorizzando il rumore del training. Devi fermarti (Early Stopping) o usare regolarizzazione (Dropout).
   C. Il Learning Rate è troppo basso. Devi aumentarlo.
   D. È normale, l'errore di validazione deve salire.

   **Spiegazione:** La divergenza tra Training Loss (che scende) e Validation Loss (che sale) è il segno classico di Overfitting: il modello perde capacità di generalizzare.

147. Durante il pre-processing di testo per un motore di ricerca, elimini le Stop Words. Tuttavia, per query come "To be or not to be", il sistema non restituisce nulla. Qual è il trade-off logico qui?
   A. Le Stop Words non servono mai, l'errore è nel motore di ricerca.
   B. Avresti dovuto usare lo Stemming invece delle Stop Words.
   C. Rimuovere le Stop Words riduce l'indice, ma si perde la capacità di cercare frasi esatte dove quelle particelle sono essenziali per il significato (Phrase Query).
   D. Bisognava usare un database NoSQL.

   **Spiegazione:** Stop Words comuni (to, be, or, not) sono inutili per la "topic detection" ma essenziali per frasi specifiche ("Phrase Queries") o quando la negazione ("not") inverte il senso.

148. Hai un attributo "Reddito" con valori da 10k a 100k e un attributo "Età" da 0 a 100. Se applichi K-Means senza normalizzare, cosa succederà logicamente?
   A. L'attributo "Reddito" dominerà il calcolo della distanza euclidea, rendendo l'"Età" irrilevante per il clustering.
   B. Nulla, K-Means è invariante alla scala.
   C. L'attributo "Età" dominerà perché ha un range più piccolo e compatto.
   D. L'algoritmo andrà in errore per overflow.

   **Spiegazione:** K-Means usa la distanza Euclidea. Una differenza di 1000 nel reddito pesa molto più di una differenza di 50 anni in età. Senza normalizzazione (z-score/min-max), il clustering segue solo il reddito.

149. La clausola SQL `HAVING` differisce da `WHERE` perché:
   A. `WHERE` filtra i gruppi dopo l'aggregazione, `HAVING` prima.
   B. `WHERE` filtra le singole righe prima dell'aggregazione, `HAVING` filtra i risultati aggregati (es. SUM(prezzo) > 100).
   C. `HAVING` è deprecato nelle versioni moderne di SQL.
   D. Sono sinonimi esatti.

   **Spiegazione:** Logica SQL fondamentale: Non puoi usare `WHERE SUM(x) > 10` perché al momento del WHERE l'aggregazione non esiste ancora. Devi usare `HAVING` dopo il `GROUP BY`.

150. In un Dendrogramma (Clustering Gerarchico), se tagli l'albero a un livello molto alto (vicino alla radice), ottieni:
   A. Pochi cluster grandi e generici.
   B. Moltissimi cluster piccoli e specifici.
   C. Nessun cluster.
   D. Esattamente K cluster dove K è la radice quadrata di N.

   **Spiegazione:** Il Dendrogramma parte dal basso (tanti punti singoli) verso l'alto (un unico cluster). Tagliare in alto significa fermarsi quando i gruppi sono pochi e grossi.

151. Se usi DBSCAN con un `Epsilon` molto piccolo e un `MinPts` molto alto, cosa accadrà logicamente ai tuoi dati?
   A. Tutti i punti diventeranno un unico grande cluster.
   B. Otterrai un ottimo clustering bilanciato.
   C. La maggior parte dei punti sarà classificata come NOISE (Outlier), perché è troppo difficile soddisfare la densità richiesta.
   D. L'algoritmo creerà cluster sferici perfetti.

   **Spiegazione:** Epsilon piccolo = raggio stretto. MinPts alto = serve tanta folla in quel raggio stretto. Condizione molto restrittiva -> quasi nessun punto diventa "Core Point" -> quasi tutto è Noise.

152. Vuoi calcolare la similarità tra due documenti di lunghezza molto diversa. Perché la Distanza Euclidea tra i vettori TF (Term Frequency) grezzi non è una buona idea?
   A. Perché i documenti lunghi avranno vettori con magnitudo molto maggiore (più parole totali), risultando "distanti" anche se parlano delle stesse cose. Meglio la Cosine Similarity.
   B. Perché la Distanza Euclidea funziona solo con numeri negativi.
   C. Perché i documenti corti hanno sempre valori negativi.
   D. È un'ottima idea, la lunghezza conta.

   **Spiegazione:** La Cosine Similarity guarda l'angolo (orientamento) dei vettori, ignorando la loro lunghezza (magnitudo). Due testi identici ma uno ripetuto 2 volte avranno coseno 1 (angolo 0) ma distanza euclidea grande.

153. Se il tuo modello di classificazione ha un alto Bias e bassa Varianza (Underfitting), quale azione ha più senso logico?
   A. Aggiungere più dati di training.
   B. Ridurre la complessità del modello (togliere feature, ridurre layer).
   C. Aumentare la complessità del modello (aggiungere feature polinomiali, più layer neuroni) per catturare meglio le relazioni.
   D. Usare una regolarizzazione L2 molto forte.

   **Spiegazione:** Alto Bias significa che il modello è "stupido", troppo semplice. Aggiungere dati a un modello stupido non aiuta. Devi renderlo più "intelligente" (complesso).

154. In un'analisi Market Basket, trovi la regola `{Pane} -> {Latte}` con Confidenza 100% ma Supporto 0.0001%. È una regola utile?
   A. Sì, è una certezza assoluta, importantissima per il business.
   B. Sì, il Supporto non conta, conta solo la Confidenza.
   C. No, perché accade troppe poche volte (probabilmente 1 o 2 transazioni su milioni) per essere statisticamente rilevante o sfruttabile commercialmente (Noise).
   D. No, la Confidenza deve essere almeno 110%.

   **Spiegazione:** Regole con supporto infinitesimale sono spesso casualità o eventi unici. Nel mining si cerca "statisticamente frequente".

155. Qual è la conseguenza logica di impostare un valore K=1 nell'algoritmo K-NN (K-Nearest Neighbors)?
   A. Il modello diventa robustissimo al rumore.
   B. Il modello diventa sensibilissimo al rumore (overfitting locale): basta un singolo outlier vicino per sbagliare la classificazione.
   C. Il modello impara la media globale dei dati.
   D. Il confine decisionale diventa una linea retta perfetta.

   **Spiegazione:** K=1 significa "copia il vicino più prossimo". Se il vicino è un outlier o un errore, classifichi errato. K più alto "liscia" il confine decisionale.

156. Se applichi una PCA (Principal Component Analysis) e mantieni le prime 2 componenti che spiegano il 95% della varianza, cosa hai ottenuto?
   A. Hai eliminato le feature meno importanti semanticamente.
   B. Hai fatto clustering.
   C. Hai selezionato le 2 colonne originali migliori.
   D. Hai proiettato i dati da N dimensioni a 2 dimensioni, preservando la massima informazione possibile, visualizzandoli su un piano.

   **Spiegazione:** La PCA crea *nuove* variabili (componenti) combinando linearmente quelle vecchie. Non seleziona colonne, comprime lo spazio.

157. Perché in una rete neurale non si inizializzano tutti i pesi a zero (Symmetry Breaking)?
   A. Perché altrimenti tutti i neuroni calcolerebbero la stessa identica funzione e lo stesso gradiente, rendendo inutile avere più neuroni.
   B. Perché lo zero causa divisione per zero.
   C. Perché lo zero è un numero sfortunato in informatica.
   D. Si può fare, è la pratica standard.

   **Spiegazione:** Se i pesi sono tutti uguali (es. 0), durante la backpropagation tutti i neuroni dello stesso strato riceveranno lo stesso aggiornamento. La rete non impara feature diverse.

158. Stai usando Naive Bayes per classificare email. Una parola "W" appare nel training set SPAM ma MAI nel training set NON-SPAM. Quando classifichi una nuova mail, la probabilità P(W|Non-Spam) sarà 0. Cosa succede a tutta la formula della probabilità e come risolvi?
   A. Non succede nulla, 0 è un valore valido.
   B. La probabilità totale diventa 0 (annullando tutto). Si risolve con lo Smoothing (Laplace Smoothing) aggiungendo un conteggio fittizio (+1).
   C. Il modello esplode.
   D. Si risolve eliminando la parola "W".

   **Spiegazione:** Essendo Naive Bayes un prodotto di probabilità, un singolo fattore 0 annulla tutto il prodotto. Lo Smoothing serve a dire "mai visto non vuol dire impossibile".

159. In SQL, qual è la differenza logica tra `COUNT(*)` e `COUNT(column_name)`?
   A. `COUNT(*)` conta tutte le righe inclusi i NULL. `COUNT(column)` conta solo le righe dove quella specifica colonna NON è NULL.
   B. Nessuna.
   C. `COUNT(*)` è più lento.
   D. `COUNT(column)` conta anche i duplicati, `COUNT(*)` no.

160. Se un algoritmo di classificazione (es. Decision Tree) crea regole del tipo "SE ID_Cliente = 123 ALLORA Compra", cos'è successo?
   A. Ha trovato un pattern molto personalizzato, ottimo.
   B. Ha fatto underfitting.
   C. È un comportamento corretto per il CRM.
   D. Ha fatto Overfitting memorizzando gli ID univoci invece di imparare pattern generalizzabili (es. Età, Residenza).

   **Spiegazione:** Usare ID univoci come predittori è l'esempio classico di "imparare a memoria". Non funzionerà mai su nuovi clienti.

161. Stai analizzando recensioni. Lemmatizzi "Best" -> "Good". Cosa perdi logicamente?
   A. Nulla, sono sinonimi.
   B. Perdi statistiche di frequenza.
   C. La distinzione grammaticale tra superlativo e positivo. Perdi l'intensità del sentimento (Sentiment Analysis).
   D. Il testo diventa illeggibile.

   **Spiegazione:** In task come la Sentiment Analysis, "Best" vale molto più di "Good". Normalizzare troppo aggressivamente appiattisce le sfumature.

162. Hai un dataset con feature categoriche (Colore: Rosso, Verde, Blu). K-Means non accetta stringhe. Converti in: Rosso=1, Verde=2, Blu=3. Qual è l'errore logico gravissimo?
   A. Stai introducendo un ordine (Blu > Verde > Rosso) e una distanza che non esistono nella realtà. K-Means penserà che Blu è più simile a Verde che a Rosso. Devi usare One-Hot Encoding.
   B. Nessun errore.
   C. Dovevi usare numeri negativi.
   D. Dovevi usare numeri decimali.

   **Spiegazione:** Label Encoding su dati nominali (senza ordine) introduce relazioni spurie che gli algoritmi geometrici interpretano falsamente.

163. Qual è l'implicazione del "Curse of Dimensionality" (Maledizione della Dimensionalità) per gli algoritmi basati sulla distanza (come KNN)?
   A. Più dimensioni ci sono, più accurata diventa la distanza.
   B. All'aumentare delle dimensioni (feature), lo spazio diventa talmente vasto e sparso che tutti i punti tendono ad essere equidistanti tra loro, rendendo il concetto di "vicino" privo di significato.
   C. Il calcolo diventa troppo veloce.
   D. Le dimensioni collassano su se stesse.

   **Spiegazione:** In alta dimensionalità la differenza tra la distanza minima e massima tende a zero. Gli algoritmi di "vicinato" smettono di funzionare bene.

164. Se un grafico "Elbow Method" per K-Means non mostra un gomito netto ma una curva dolce e continua, cosa deduci logicamente?
   A. Che K deve essere infinito.
   B. Che devi usare K=1.
   C. Che hai sbagliato il grafico.
   D. Che i dati non hanno una struttura a cluster ben definita e separata (forse sono distribuiti uniformemente).

   **Spiegazione:** L'assenza di gomito indica che aggiungere cluster migliora la varianza in modo marginale e costante, non c'è un punto di "svolta" dove i gruppi si consolidano.

165. In una Cross-Validation a 10 fold, se ottieni performance drasticamente diverse in ogni fold (es. 90%, 50%, 95%...), cosa suggerisce?
   A. Che il modello è robusto.
   B. Che i dati non sono distribuiti in modo omogeneo (il dataset potrebbe essere ordinato temporalmente o per classe prima dello split, o i campioni sono troppo pochi/rumorosi). Instabilità.
   C. Che hai usato un computer difettoso.
   D. Che devi fare la media e basta.

   **Spiegazione:** L'alta varianza tra fold suggerisce che il modello dipende troppo da QUALI dati vede, quindi non è stabile. Bisogna mescolare (shuffle) meglio o raccogliere più dati.

166. Vuoi prevedere il prezzo delle case. Usi la Regressione Lineare. I residui (Errori) mostrano un pattern a forma di "U" quando plottati. Cosa significa?
   A. Che i dati sono perfetti.
   B. Che il modello lineare non è sufficiente. La relazione tra le variabili è probabilmente non-lineare (quadratica).
   C. Che hai degli outlier.
   D. Che devi usare la classificazione.

   **Spiegazione:** Se i residui non sono casuali ma hanno un pattern, il modello non sta catturando un'informazione sistematica (la curvatura).

167. Nel calcolo di TF-IDF, se una parola appare in TUTTI i documenti del corpus, quanto varrà il suo IDF (Inverse Document Frequency)?
   A. Infinito.
   B. Uno.
   C. Massimo.
   D. Zero (o molto vicino a zero), rendendo il peso della parola nullo.

   **Spiegazione:** IDF = log(N/df). Se df=N (appare in tutti), log(1) = 0. Giustamente: se una parola è ovunque (es. "il"), non distingue nessun documento.

168. Stai analizzando i log di accesso. Un utente fa login alle 10:00 da Roma e alle 10:05 da New York. Un sistema a regole (Rule-based) lo flaga subito. Un modello statistico medio potrebbe non farlo se addestrato solo su "numero di login". Qual è il vantaggio dell'approccio a regole qui?
   A. Le regole possono catturare logicamente vincoli fisici ("Impossibile viaggiare così veloce") che un modello statistico puro potrebbe ignorare se non ha la feature "velocità di spostamento" esplicita.
   B. Nessuno.
   C. Il modello statistico è sempre superiore.
   D. Le regole sono più veloci.

   **Spiegazione:** A volte la "Domain Knowledge" esplicita (regole fisiche) batte l'inferenza statistica cieca su feature incomplete.

169. Perché si usa lo "Stratified Sampling" invece del "Random Sampling" per creare il Test Set in problemi di classificazione sbilanciata?
   A. Per mescolare meglio i dati.
   B. Per fare prima.
   C. Per garantire che la proporzione delle classi (es. 99% sani, 1% malati) sia mantenuta identica sia nel Train che nel Test set, evitando che il Test set finisca per non avere nessun "malato".
   D. Per eliminare i duplicati.

   **Spiegazione:** Casualmente potresti pescare solo sani nel test set. Stratified forza la presenza dell'1% di malati anche nel test.

170. Qual è l'effetto di avere variabili altamente correlate (Multicollinearità) in una Regressione Lineare Multipla?
   A. Rende instabili le stime dei coefficienti (pesi): piccoli cambiamenti nei dati possono ribaltare il segno o il valore dei coefficienti, rendendo difficile l'interpretazione (Feature Importance).
   B. Aumenta la precisione.
   C. Blocca il calcolo.
   D. Nessun effetto.

   **Spiegazione:** Se X1 e X2 dicono la stessa cosa, il modello non sa se attribuire l'effetto a X1 o X2 (infinite soluzioni matematiche), rendendo i pesi inaffidabili.

171. Stai usando un algoritmo genetico per ottimizzare una funzione. La popolazione converge istantaneamente dopo 2 generazioni su una soluzione mediocre e non migliora più. Cosa è successo?
   A. L'algoritmo è velocissimo e ha trovato l'ottimo globale.
   B. Premature Convergence (Convergenza Prematura). La diversità genetica è persa troppo presto (minimo locale).
   C. La mutazione è troppo alta.
   D. Hai troppi individui.

   **Spiegazione:** Come l'inincrocio in biologia: se tutti sono uguali, l'evoluzione si ferma. Bisogna aumentare la mutazione o la dimensione della popolazione.

172. In un database colonnare (rispetto a uno 'row-based'), quale operazione è logicamente molto più veloce?
   A. Inserire una nuova riga completa (INSERT).
   B. Recuperare tutti i dati di un singolo utente specifico.
   C. Calcolare la media di una singola colonna su miliardi di righe (es. AVG(Salary)).
   D. Aggiornare una riga.

   **Spiegazione:** I DB colonnari memorizzano i dati di una colonna vicini su disco. Leggere una colonna sola è sequenziale e velocissimo. Leggere una riga richiede salti.

---

### RISPOSTE CORRETTE

1:C, 2:B, 3:A, 4:B, 5:C, 6:D, 7:B, 8:B, 9:B, 10:D,
11:C, 12:A, 13:C, 14:B, 15:A, 16:A, 17:C, 18:B, 19:D, 20:A,
21:D, 22:C, 23:D, 24:C, 25:B, 26:D, 27:B, 28:A, 29:A, 30:D,
31:D, 32:A, 33:B, 34:C, 35:D, 36:D, 37:D, 38:C, 39:A, 40:D,
41:B, 42:D, 43:C, 44:C, 45:B, 46:D, 47:C, 48:A, 49:D, 50:C,
51:A, 52:B, 53:A, 54:D, 55:C, 56:A, 57:A, 58:C, 59:A, 60:A,
61:B, 62:D, 63:D, 64:C, 65:A, 66:C, 67:A, 68:D, 69:D, 70:D,
71:C, 72:D, 73:A, 74:B, 75:B, 76:B, 77:A, 78:A, 79:A, 80:C,
81:A, 82:A, 83:A, 84:C, 85:B, 86:B, 87:D, 88:C, 89:D, 90:A,
91:A, 92:A, 93:C, 94:B, 95:C, 96:A, 97:B, 98:C, 99:A, 100:D,
101:D, 102:B, 103:C, 104:D, 105:C, 106:A, 107:C, 108:A, 109:C, 110:D,
111:B, 112:C, 113:D, 114:A, 115:C, 116:B, 117:B, 118:C, 119:D, 120:C,
121:C, 122:A, 123:A, 124:B, 125:D, 126:C, 127:A, 128:D, 129:D, 130:B,
131:C, 132:C, 133:A, 134:B, 135:A, 136:B, 137:B, 138:C, 139:A, 140:B,
141:B, 142:B, 143:C, 144:A, 145:D, 146:B, 147:C, 148:A, 149:B, 150:A,
151:C, 152:A, 153:C, 154:C, 155:B, 156:D, 157:A, 158:B, 159:A, 160:D,
161:C, 162:A, 163:B, 164:D, 165:B, 166:B, 167:D, 168:A, 169:C, 170:A,
171:B, 172:C
